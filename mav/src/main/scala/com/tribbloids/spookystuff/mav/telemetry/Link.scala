package com.tribbloids.spookystuff.mav.telemetry

import com.tribbloids.spookystuff.mav.actions._
import com.tribbloids.spookystuff.mav.dsl.{LinkFactories, LinkFactory}
import com.tribbloids.spookystuff.mav.{MAVConf, ReinforcementDepletedException}
import com.tribbloids.spookystuff.session.python._
import com.tribbloids.spookystuff.session.{LocalCleanable, Session}
import com.tribbloids.spookystuff.utils.{SpookyUtils, TreeException}
import com.tribbloids.spookystuff.{PyInterpreterException, SpookyContext, caching}
import org.slf4j.LoggerFactory

import scala.util.Try

case class Endpoint(
                     // remember, one drone can have several telemetry
                     // endpoints: 1 primary and several backups (e.g. text message-based)
                     // TODO: implement telemetry backup mechanism, can use MAVproxy's multiple master feature
                     connStrs: Seq[String], // [protocol]:ip:port;[baudRate]
                     baudRate: Int = MAVConf.DEFAULT_BAUDRATE,
                     ssid: Int = MAVConf.EXECUTOR_SSID,
                     frame: Option[String] = None,
                     name: String = "DRONE"
                   ) extends CaseInstanceRef with SingletonRef with LocalCleanable {

  def connStr = connStrs.head
}

object Link {

  // max 1 per task/thread.
  val driverLocal: caching.ConcurrentMap[PythonDriver, Link] = caching.ConcurrentMap()

  // connStr -> (link, isBusy)
  // only 1 allowed per connStr, how to enforce?
  val existing: caching.ConcurrentMap[Endpoint, LinkWithContext] = caching.ConcurrentMap()

  // won't be used to create any link before its status being recovered by ping daemon.
  val blacklist: caching.ConcurrentSet[Endpoint] = caching.ConcurrentSet()

  def getOrInitialize(
                       candidates: Seq[Endpoint],
                       factory: LinkFactory,
                       session: Session,
                       locationOpt: Option[Location] = None
                     ): Link = {
    session.initializeDriverIfMissing {
      getOrCreate(candidates, factory, session, locationOpt)
    }
  }

  /**
    * create a telemetry link based on the following order:
    * if one is already created in the same task, reuse it
    * if one is created in a previous task and not busy, use it. The busy status is controlled by whether it has an active python driver.
    *   - if its generated by an obsolete ProxyFactory, terminate the link and immediately recreate a new one with the new ProxyFactory,
    *     being created means the drone is already in the air, and can be deployed much faster
    * * if multiple are created by previous tasks and not busy, use the one that is closest to the first waypoint * (not implemented yet)
    * If none of the above exists, create one from candidates from scratch
    * remember: once the link is created its proxy is bind to it until death.
    */
  def getOrCreate(
                   candidates: Seq[Endpoint],
                   factory: LinkFactory,
                   session: Session,
                   locationOpt: Option[Location] = None
                 ): Link = {

    val local = driverLocal
      .get(session.pythonDriver)

    local.foreach {
      link =>
        LoggerFactory.getLogger(this.getClass).info(
          s"Using existing Link ${link.endpoint.connStr} with the same driver"
        )
    }

    val result = local
      .getOrElse {
        val newLink = recommissionIdle(candidates, factory, session, locationOpt)
          .getOrElse {
            selectAndCreate(candidates, factory, session)
          }
        try {
          newLink.link.Py(session)
        }
        catch {
          case e: Throwable =>
            newLink.clean()
            throw e
        }

        newLink.link
      }
    result
  }

  // CAUTION: this will refit the telemetry link with new Proxy and clean the old one if ProxyFactory is different.
  def recommissionIdle(
                        candidates: Seq[Endpoint],
                        factory: LinkFactory,
                        session: Session,
                        locationOpt: Option[Location] = None
                      ): Option[LinkWithContext] = {

    val result = this.synchronized {
      val existingCandidates: Seq[LinkWithContext] = candidates.collect {
        Function.unlift {
          endpoint =>
            existing.get(endpoint)
        }
      }

      val idleLinks = existingCandidates.filter {
        link =>
          link.link.isIdle
      }

      //TODO: find the closest one!
      val idleLinkOpt = idleLinks.headOption

      idleLinkOpt match {
        case Some(idleLink) =>
          val recommissioned = {
            if (LinkFactories.canCreate(factory, idleLink)) {
              LoggerFactory.getLogger(this.getClass).info {
                s"Recommissioning telemetry Link for ${idleLink.link.endpoint.connStr} with old proxy"
              }
              idleLink.link.onHold = true
              idleLink
            }
            else {
              idleLink.clean()
              // recreate proxy
              val link = factory.apply(idleLink.link.endpoint)
              link.onHold = true
              LoggerFactory.getLogger(this.getClass).info {
                s"Recommissioning telemetry Link for ${link.endpoint.connStr} with new proxy"
              }
              LinkWithContext(
                link,
                session.spooky,
                factory
              )
            }
          }

          Some(recommissioned)
        case None =>
          LoggerFactory.getLogger(this.getClass).info{
            if (existingCandidates.isEmpty) {
              val msg = s"No existing telemetry Link for ${candidates.map(_.connStr).mkString("[", ", ", "]")}, existing links are:"
              val hint = Link.existing.keys.toList.map(_.connStr)
                .mkString("[", ", ", "]")
              msg + "\n" + hint
            }
            else {
              existingCandidates.map {
                link =>
                  assert(!link.link.isIdle)
                  s"${link.link.endpoint.connStr} is busy"
              }
                .mkString("\n")
            }
          }
          None
      }
    }

    result
  }

  def selectAndCreate(
                       candidates: Seq[Endpoint],
                       factory: LinkFactory,
                       session: Session
                     ): LinkWithContext = {

    val newLink = this.synchronized {
      val endpointOpt = candidates.find {
        v =>
          !existing.contains(v) &&
            !blacklist.contains(v)
      }
      val endpoint = endpointOpt
        .getOrElse(
          throw new ReinforcementDepletedException(
            candidates.map {
              candidate =>
                if (blacklist.contains(candidate)) s"${candidate.connStr} is unreachable"
                else s"${candidate.connStr} is busy"
            }
              .mkString(", ")
          )
        )

      create(endpoint, factory, session.spooky)
    }
    newLink
  }

  def create(
              endpoint: Endpoint,
              factory: LinkFactory,
              spooky: SpookyContext
            ): LinkWithContext = {

    val link = factory.apply(endpoint)
    link.onHold = true
    LinkWithContext(
      link,
      spooky,
      factory
    )
  }

  class PyBindingImpl(
                       override val ref: Link,
                       override val driver: PythonDriver,
                       override val spookyOpt: Option[SpookyContext]
                     ) extends com.tribbloids.spookystuff.session.python.PyBinding(ref, driver, spookyOpt) {

    $Helpers.autoStart()
    Link.driverLocal += driver -> ref

    override def cleanImpl(): Unit = {
      super.cleanImpl()
      val localOpt = Link.driverLocal.get(driver)
      localOpt.foreach {
        v =>
          if (v eq this.ref)
            Link.driverLocal -= driver
      }
    }

    object $Helpers {
      var isStarted: Boolean = false

      def _startDaemons(): Unit = {
        if (!isStarted) {
          ref.proxyOpt.foreach {
            _.managerPy.start()
          }
          ref.primary._Py(driver, spookyOpt).start()
        }
        isStarted = true
      }

      def stopDaemons(): Unit = {
        ref.primary._Py(driver, spookyOpt).stop()
        ref.proxyOpt.foreach {
          _.managerPy.stop()
        }
        isStarted = false
      }

      def withDaemonsUp[T](fn: => T) = {
        try {
          _startDaemons()
          fn
        }
        catch {
          case e: Throwable =>
            stopDaemons()
            throw e
        }
      }

      // will retry 6 times, try twice for Vehicle.connect() in python, if failed, will restart proxy and try again (3 times).
      // after all attempts failed will stop proxy and add endpoint into blacklist.
      def autoStart(): Unit = try {
        val retries = spookyOpt.map(
          spooky =>
            spooky.conf.submodules.get[MAVConf]().connectionRetries
        )
          .getOrElse(MAVConf.CONNECTION_RETRIES)
        SpookyUtils.retry(retries) {
          withDaemonsUp[Unit]()
        }
      }
      catch {
        case e: PyInterpreterException => //this indicates a possible port conflict
          //TODO: enable after ping daemon

          try {
            ref.detectPortConflicts(Option(e.cause).toSeq)
          }
          catch {
            case ee: TreeException =>
              throw e.copy(
                cause = ee
              )
          }
          throw e.copy(code = e.code + "\n\n\t### No port conflict detected ###")
      }

      def getCurrentLocation: LocationGlobal = {

        val locations = ref.primary._Py(driver, spookyOpt).vehicle.location
        val global = locations.global_frame.$MSG.get.cast[LocationGlobal]
        //      val globalRelative = locations.global_relative_frame.$MSG.get.cast[LocationGlobalRelative]
        //      val local = locations.local_frame.$MSG.get.cast[LocationLocal]

        //      val result = LocationBundle(
        //        global,
        //        globalRelative,
        //        local
        //      )
        ref._current = Some(global)
        global
      }
    }
  }
}

/**
to keep a drone in the air, a python daemon process D has to be constantly running to
supervise task-irrelevant path planning (e.g. RTL/Position Hold/Avoidance).
This process outlives each task. Who launches D? how to ensure smooth transitioning
of control during Partition1 => D => Partition2 ? Can they share the same
Connection / Endpoint / Proxy ? Do you have to make them picklable ?

GCS:UDP:xxx ------------------------> Proxy:TCP:xxx -> Drone
                                   /
TaskProcess -> Connection:UDP:xx -/
            /
DaemonProcess   (can this be delayed to be implemented later? completely surrender control to GCS after Altitude Hold)
  is Vehicle picklable? if yes then that changes a lot of things.
  but if not ...
    how to ensure that an interpreter can takeover and get the same vehicle?
  */
//TODO: this should be a dummy Instance that can be bind to a python driver but cannot be initialized.
case class Link(
                 endpoint: Endpoint,
                 executorOuts: Seq[String] = Nil, // cannot have duplicates
                 gcsOuts: Seq[String] = Nil
               ) extends NoneRef with SingletonRef with LocalCleanable {

  if (executorOuts.isEmpty) assert(gcsOuts.isEmpty, "No endpoint for executor")

  object URI {

  }

  val outs: Seq[String] = executorOuts ++ gcsOuts
  val allURI = (endpoint.connStrs ++ outs).distinct

  val endpointsForExecutor = if (executorOuts.isEmpty) {
    Seq(endpoint)
  }
  else {
    executorOuts.map {
      out =>
        endpoint.copy(
          connStrs = Seq(out)
        )
    }
  }
  //always initialized in Python when created from companion object
  val primary: Endpoint = endpointsForExecutor.head

  val endpointsForGCS = {
    gcsOuts.map {
      out =>
        endpoint.copy(
          connStrs = Seq(out)
        )
    }
  }

  val allEndpoints: Seq[Endpoint] = (Seq(endpoint) ++ endpointsForExecutor ++ endpointsForGCS).distinct

  override type Binding = Link.PyBindingImpl

  /**
    * set true to block being used by another thread before its driver is created
    */
  @volatile var onHold: Boolean = true
  def isIdle: Boolean = {
    !onHold && primary.validDriverToBindings.isEmpty
  }

  //mnemonic
  @volatile private var _proxyOpt: Option[Proxy] = _
  def proxyOpt: Option[Proxy] = Option(_proxyOpt).getOrElse {
    this.synchronized {
      _proxyOpt = if (outs.isEmpty) None
      else {
        val proxy = Proxy(
          endpoint.connStr,
          outs,
          endpoint.baudRate,
          name = endpoint.name
        )
        Some(proxy)
      }
      _proxyOpt
    }
  }

  var home: Option[LocationGlobal] = None

  var _current: Option[LocationGlobal] = None
  def current = _current

  def wContext(
                spooky: SpookyContext,
                factory: LinkFactory
              ) = LinkWithContext(
    this,
    spooky,
    factory
  )

  //  def sublink(index: Int, ssid: Int): Sublink = {
  //    val result = new Sublink(this,index, ssid)
  //    result.detectPortConflicts()
  //    result
  //  }

  override protected def newPy(driver: PythonDriver, spookyOpt: Option[SpookyContext]): Link.PyBindingImpl = {
    val result = new Link.PyBindingImpl(this, driver, spookyOpt)
    onHold = false
    result
  }

  def _distinctEndpoint: Boolean = true

  def detectPortConflicts(causes: Seq[Throwable] = Nil): Unit = {
    val existing = Link.existing.values.toList // remember to clean up the old one to create a new one
    val s1 = if (_distinctEndpoint){
      val notThis = existing.filterNot(_.link eq this)
      val ss1 = Seq(
        Try(assert(Link.existing.get(endpoint).forall(_.link eq this), s"Conflict: endpoint index ${endpoint.connStr} is already used")),
        Try(assert(!notThis.exists(_.link.endpoint.connStr == endpoint.connStr), s"Conflict: endpoint ${endpoint.connStr} is already used"))
      )
      val allConnStrs: Map[String, Int] = existing.flatMap(_.link.endpoint.connStrs)
        .groupBy(identity)
        .mapValues(_.size)
      val ss2 = allConnStrs.toSeq.map {
        tuple =>
          Try(assert(tuple._2 == 1, s"${tuple._2} endpoints has identical uri ${tuple._1}"))
      }
      ss1 ++ ss2
    }
    else {
      Nil
    }
    val allExecutorOuts: Map[String, Int] = existing.flatMap(_.link.executorOuts)
      .groupBy(identity)
      .mapValues(_.size)
    val s = s1 ++ allExecutorOuts.toSeq.map {
      tuple =>
        Try(assert(tuple._2 == 1, s"${tuple._2} executor out has identical uri ${tuple._1}"))
    }

    TreeException.&&&(s, extra = causes)
  }

  var isDryrun = false
  //finalizer may kick in and invoke it even if its in Link.existing
  override protected def cleanImpl(): Unit = {

    super.cleanImpl()
    allEndpoints.foreach(_.clean())
    Option(_proxyOpt).flatten.foreach(_.clean())
    //TODO: move to LinkWithContext?
    val existingOpt = Link.existing.get(this.endpoint)
    existingOpt.foreach {
      v =>
        if (v.link eq this)
          Link.existing -= this.endpoint
        else {
          if (!isDryrun) throw new AssertionError("THIS IS NOT A DRYRUN OBJECT! SO ITS CREATED ILLEGALLY!")
        }
    }
    //otherwise its a zombie Link created by LinkFactories.canCreate
  }
}

///**
//  * Only used in test for GCS control mimic.
//  */
//class Sublink(
//               parent: Link,
//               index: Int,
//               ssid: Int
//             ) extends Link(
//  parent.endpoint,
//  {
//    require(index >= 1)
//    val primary = parent.outs(index)
//    Seq(primary) ++ parent.outs.filterNot(_ == primary)
//  },
//  ssid
//) {
//
//  override def _distinctEndpoint = false
//
//  override def className = classOf[Link].getCanonicalName
//  override def proxyOpt: Option[Proxy] = parent.proxyOpt
//  override def current: Option[LocationGlobal] = parent.current
//}

case class LinkWithContext(
                            link: Link,
                            spooky: SpookyContext,
                            factory: LinkFactory
                          ) extends LocalCleanable {
  try {
    link.detectPortConflicts()

    Link.existing += link.endpoint -> this
    spooky.metrics.linkCreated += 1
  }
  catch {
    case e: Throwable =>
      this.clean()
      throw e
  }

  protected override def cleanImpl(): Unit = {

    link.clean()
    spooky.metrics.linkDestroyed += 1
  }
}