package com.tribbloids.spookystuff.mav.telemetry

import com.tribbloids.spookystuff.{SpookyContext, caching}
import com.tribbloids.spookystuff.mav.{MAVConf, ReinforcementDepletedException}
import com.tribbloids.spookystuff.session.python._
import com.tribbloids.spookystuff.session.{LocalCleanable, Session}
import com.tribbloids.spookystuff.utils.{SpookyUtils, TreeException}
import org.slf4j.LoggerFactory

import scala.util.{Failure, Try}

case class Endpoint(
                     // remember, one drone can have several telemetry
                     // endpoints: 1 primary and several backups (e.g. text message-based)
                     // TODO: implement telemetry backup mechanism, can use MAVproxy's multiple master feature
                     connStrs: Seq[String],
                     vehicleTypeOpt: Option[String] = None
                   ) extends CaseInstanceRef {

  def connStr = connStrs.head
}

object Link extends StaticRef {

  // max 1 per task/thread.
  val driverLocal: caching.ConcurrentMap[PythonDriver, Link] = caching.ConcurrentMap()

  // connStr -> (link, isBusy)
  // only 1 allowed per connStr, how to enforce?
  // TODO: change to Endpoint -> tuple mapping?
  val existing: caching.ConcurrentMap[String, LinkWithContext] = caching.ConcurrentMap()

  // won't be used to create any link before its status being recovered by ping daemon.
  val blacklist: caching.ConcurrentSet[String] = caching.ConcurrentSet()

  //in the air but unused
  //  def idle: Map[String, (ProxyFactory, Link)] = existing.filter {
  //    tuple =>
  //      !blacklist.contains(tuple._1) &&
  //        !tuple._2._2.isBusy
  //  }

  def getOrInitialize(
                       candidates: Seq[Endpoint],
                       factory: LinkFactory,
                       session: Session
                     ): Link = {
    session.initializeDriverIfMissing {
      getOrCreate(candidates, factory, session)
    }
  }

  /**
    * create a telemetry link based on the following order:
    * if one is already created in the same task, reuse it
    * if one is created in a previous task and not busy, use it. The busy status is controlled by whether it has an active python driver.
    *   - if its generated by an obsolete ProxyFactory, terminate the link and immediately recreate a new one with the new ProxyFactory,
    *     being created means the drone is already in the air, and can be deployed much faster
    * * if multiple are created by previous tasks and not busy, use the one that is closest to the first waypoint * (not implemented yet)
    * If none of the above exists, create one from candidates from scratch
    * remember: once the link is created its proxy is bind to it until death.
    */
  def getOrCreate(
                   candidates: Seq[Endpoint],
                   factory: LinkFactory,
                   session: Session
                 ): Link = {

    val local = driverLocal
      .get(session.pythonDriver)

    local.foreach {
      link =>
        LoggerFactory.getLogger(this.getClass).info(
          s"Using existing Link ${link.endpoint.connStr} with the same driver"
        )
    }

    val result = local
      .getOrElse {
        SpookyUtils.retry(2) {
          val newLink = refitIdle(candidates, factory, session).getOrElse {
            elect(candidates, factory, session)
          }
          try {
            newLink.link.Py(session)
          }
          catch {
            case e: Throwable =>
              newLink.clean()
              throw e
          }

          newLink.link
        }
      }
    result
  }

  // CAUTION: this will refit the telemetry link with new Proxy and clean the old one if ProxyFactory is different.
  def refitIdle(
                 candidates: Seq[Endpoint],
                 factory: LinkFactory,
                 session: Session
               ): Option[LinkWithContext] = {

    val result = this.synchronized {
      val existingCandidates: Seq[LinkWithContext] = candidates.collect {
        Function.unlift {
          endpoint =>
            existing.get(endpoint.connStr)
        }
      }

      val idleLinkOpt = existingCandidates.find {
        link =>
          link.link.isIdle
      }

      idleLinkOpt match {
        case Some(idleLink) =>
          val refittedLink = {
            if (factory.canCreate(idleLink)) {
              LoggerFactory.getLogger(this.getClass).info {
                s"Refitting telemetry Link for ${idleLink.link.endpoint.connStr} with old proxy"
              }
              idleLink.link.putOnHold()
              idleLink
            }
            else {
              idleLink.clean()
              // recreate proxy
              val link = factory.apply(idleLink.link.endpoint).putOnHold()
              LoggerFactory.getLogger(this.getClass).info {
                s"Refitting telemetry Link for ${link.endpoint.connStr} with new proxy"
              }
              LinkWithContext(
                link,
                session.spooky,
                factory
              )
            }
          }

          Some(refittedLink)
        case None =>
          LoggerFactory.getLogger(this.getClass).info{
            if (existingCandidates.isEmpty) {
              s"No existing telemetry Link for ${candidates.map(_.connStr).mkString("[", ", ", "]")}"
            }
            else {
              existingCandidates.map {
                link =>
                  assert(!link.link.isIdle)
                  s"${link.link.endpoint.connStr} is busy"
              }
                .mkString("\n")
            }
          }
          None
      }
    }

    result
  }

  def elect(
             candidates: Seq[Endpoint],
             factory: LinkFactory,
             session: Session
           ): LinkWithContext = {

    val newLink = this.synchronized {
      val endpointOpt = candidates.find {
        v =>
          !existing.contains(v.connStr) &&
            !blacklist.contains(v.connStr)
      }
      val endpoint = endpointOpt
        .getOrElse(
          throw new ReinforcementDepletedException(
            candidates.map {
              candidate =>
                if (blacklist.contains(candidate.connStr)) s"${candidate.connStr} is unreachable"
                else s"${candidate.connStr} is busy"
            }
              .mkString(", ")
          )
        )

      create(endpoint, factory, session.spooky)
    }
    newLink
  }

  def create(
              endpoint: Endpoint,
              factory: LinkFactory,
              spooky: SpookyContext
            ): LinkWithContext = {

    val link = factory.apply(endpoint).putOnHold()
    LinkWithContext(
      link,
      spooky,
      factory
    )
  }

  class Binding(
                 override val ref: Link,
                 override val driver: PythonDriver,
                 override val spookyOpt: Option[SpookyContext]
               ) extends PyBinding(ref, driver, spookyOpt) {

    Helper.autoStart()
    Link.driverLocal += driver -> ref

    private object Helper {
      // will retry 6 times, try twice for Vehicle.connect() in python, if failed, will restart proxy and try again (3 times).
      // after all attempts failed will add endpoint into blacklist and stop proxy
      def autoStart(): String = try {

        var needRestart = false
        val retries = spookyOpt.map(
          spooky =>
            spooky.conf.submodules.get[MAVConf]().connectionRetries
        ).getOrElse(1)
        SpookyUtils.retry(retries) {
          try {
            ref.proxyOpt.foreach {
              proxy =>
                if (needRestart) {
                  proxy.managerPy.restart()
                }
                else {
                  proxy.managerPy.start()
                }
            }
            val result = Binding.this.start().strOpt.get
            result
          }
          finally {
            needRestart = true
          }
        }
      }
      catch {
        case e: Throwable =>
          ref.proxyOpt.foreach(_.managerPy.stop())
          //TODO: enable after ping daemon
          ref.detectConflict(Seq(e))
          throw e

        //        LoggerFactory.getLogger(this.getClass).error(
        //          s"${ref.endpoint.connStr} is down, adding to blacklist"
        //        )
        //            Link.blacklist += endpoint.connStr
      }
    }

    //  override def cleanImpl(): Unit = {
    //
    //    super.cleanImpl()
    //    Link.driverLocal -= driver
    //  }
  }
}

/**
to keep a drone in the air, a python daemon process D has to be constantly running to
supervise task-irrelevant path planning (e.g. RTL/Position Hold/Avoidance).
This process outlives each task. Who launches D? how to ensure smooth transitioning
of control during Partition1 => D => Partition2 ? Can they share the same
Connection / Endpoint / Proxy ? Do you have to make them picklable ?

GCS:UDP:xxx ------------------------> Proxy:TCP:xxx -> Drone
                                   /
TaskProcess -> Connection:UDP:xx -/
            /
DaemonProcess   (can this be delayed to be implemented later? completely surrender control to GCS after Altitude Hold)
  is Vehicle picklable? if yes then that changes a lot of things.
  but if not ...
    how to ensure that an interpreter can takeover and get the same vehicle?
  */
case class Link private[telemetry](
                                    endpoint: Endpoint,
                                    outs: Seq[String],
                                    name: String = "DRONE"
                                  ) extends CaseInstanceRef with LocalCleanable {

  //mnemonic
  @volatile var _proxyOpt: Option[Proxy] = _

  def proxyOpt: Option[Proxy] = Option(_proxyOpt).getOrElse {
    this.synchronized {
      _proxyOpt = if (outs.isEmpty) None
      else {
        val proxy = Proxy(
          endpoint.connStr,
          outs,
          name
        )
        Some(proxy)
      }
      _proxyOpt
    }
  }

  def wContext(
                spooky: SpookyContext,
                factory: LinkFactory
              ) = LinkWithContext(
    this,
    spooky,
    factory
  )

  /**
    * no duplication due to port conflicts!
    */
  lazy val uri: String = outs.headOption.getOrElse(endpoint.connStr)

  override def _Py(driver: PythonDriver, spookyOpt: Option[SpookyContext]): Link.Binding = {
    validDriverToBindings.get(driver)
      .map(_.asInstanceOf[Link.Binding])
      .getOrElse {
        assert(Link.validDriverToBindings.isEmpty, "Link can only be bind to one driver")
        val result = new Link.Binding(this, driver, spookyOpt)
        onHold = false
        result
      }
  }

  /**
    * set true to block being used by another thread before its driver is created
    */
  var onHold: Boolean = true

  def putOnHold(): this.type = {
    this.onHold = true
    this
  }

  def isIdle: Boolean = {
    !onHold && validDriverToBindings.isEmpty
  }

  def detectConflict(causes: Seq[Throwable] = Nil): Unit = {
    val c1 = Link.existing.get(endpoint.connStr).forall(_ eq this)
    val existing = Link.existing.values // remember to clean up the old one to create a new one
    val c2 = existing.filter(_.link.endpoint.connStr == endpoint.connStr).forall(_ eq this)
    val c3 = existing.filter(_.link.uri == uri).forall(_ eq this)

    TreeException.&&&(
      causes.map(v => Failure(v)) ++
        Seq(
          Try(assert(c1, s"Conflict: endpoint (index) ${endpoint.connStr} is already used")),
          Try(assert(c2, s"Conflict: endpoint ${endpoint.connStr} is already used")),
          Try(assert(c3, s"Conflict: uri $uri is already used"))
        ))
  }

  var isDryrun = false
  //finalizer may kick in and invoke it even if its in Link.existing
  override protected def cleanImpl(): Unit = {

    super.cleanImpl()
    Option(_proxyOpt).flatten.foreach(_.clean())
    val existingOpt = Link.existing.get(this.endpoint.connStr)
    existingOpt.foreach {
      v =>
        if (v.link eq this)
          Link.existing -= this.endpoint.connStr
        else {
          if (!isDryrun) throw new AssertionError("THIS IS NOT A DRYRUN OBJECT! SO ITS CREATED ILLEGALLY!")
        }
    }
    //otherwise its a zombie Link created by LinkFactories.canCreate
  }
}

case class LinkWithContext(
                            link: Link,
                            spooky: SpookyContext,
                            factory: LinkFactory
                          ) extends LocalCleanable {
  try {
    link.detectConflict()

    Link.existing += link.endpoint.connStr -> this
    spooky.metrics.linkCreated += 1
  }
  catch {
    case e: Throwable =>
      this.clean()
      throw e
  }

  protected override def cleanImpl(): Unit = {

    link.clean()
    spooky.metrics.linkDestroyed += 1
  }
}